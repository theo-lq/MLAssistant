{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of use of the Assistant class\n",
    "\n",
    "We will use a NBA player draft dataset, and the goal is to predict whether each player will stay more than 5 years in the league.\n",
    "We will perform standard level of machine learning since it is not the main goal of the notebook. We want to show how one can use the class designed, and how it can save time and code therefore errors and debugging hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>GP</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PTS</th>\n",
       "      <th>FGM</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>3P Made</th>\n",
       "      <th>3PA</th>\n",
       "      <th>3P%</th>\n",
       "      <th>...</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FT%</th>\n",
       "      <th>OREB</th>\n",
       "      <th>DREB</th>\n",
       "      <th>REB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>TARGET_5Yrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brandon Ingram</td>\n",
       "      <td>36</td>\n",
       "      <td>27.4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>7.6</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>69.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andrew Harrison</td>\n",
       "      <td>35</td>\n",
       "      <td>26.9</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>23.5</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>76.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JaKarr Sampson</td>\n",
       "      <td>74</td>\n",
       "      <td>15.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>42.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Malik Sealy</td>\n",
       "      <td>58</td>\n",
       "      <td>11.6</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>42.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>68.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matt Geiger</td>\n",
       "      <td>48</td>\n",
       "      <td>11.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>52.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>67.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tony Bennett</td>\n",
       "      <td>75</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>32.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>73.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Don MacLean</td>\n",
       "      <td>62</td>\n",
       "      <td>10.9</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>43.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>81.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tracy Murray</td>\n",
       "      <td>48</td>\n",
       "      <td>10.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>41.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>87.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Duane Cooper</td>\n",
       "      <td>65</td>\n",
       "      <td>9.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>39.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>71.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dave Johnson</td>\n",
       "      <td>42</td>\n",
       "      <td>8.5</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>38.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>67.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  GP   MIN  PTS  FGM  FGA   FG%  3P Made  3PA   3P%  ...  \\\n",
       "0   Brandon Ingram  36  27.4  7.4  2.6  7.6  34.7      0.5  2.1  25.0  ...   \n",
       "1  Andrew Harrison  35  26.9  7.2  2.0  6.7  29.6      0.7  2.8  23.5  ...   \n",
       "2   JaKarr Sampson  74  15.3  5.2  2.0  4.7  42.2      0.4  1.7  24.4  ...   \n",
       "3      Malik Sealy  58  11.6  5.7  2.3  5.5  42.6      0.1  0.5  22.6  ...   \n",
       "4      Matt Geiger  48  11.5  4.5  1.6  3.0  52.4      0.0  0.1   0.0  ...   \n",
       "5     Tony Bennett  75  11.4  3.7  1.5  3.5  42.3      0.3  1.1  32.5  ...   \n",
       "6      Don MacLean  62  10.9  6.6  2.5  5.8  43.5      0.0  0.1  50.0  ...   \n",
       "7     Tracy Murray  48  10.3  5.7  2.3  5.4  41.5      0.4  1.5  30.0  ...   \n",
       "8     Duane Cooper  65   9.9  2.4  1.0  2.4  39.2      0.1  0.5  23.3  ...   \n",
       "9     Dave Johnson  42   8.5  3.7  1.4  3.5  38.3      0.1  0.3  21.4  ...   \n",
       "\n",
       "   FTA   FT%  OREB  DREB  REB  AST  STL  BLK  TOV  TARGET_5Yrs  \n",
       "0  2.3  69.9   0.7   3.4  4.1  1.9  0.4  0.4  1.3          0.0  \n",
       "1  3.4  76.5   0.5   2.0  2.4  3.7  1.1  0.5  1.6          0.0  \n",
       "2  1.3  67.0   0.5   1.7  2.2  1.0  0.5  0.3  1.0          0.0  \n",
       "3  1.3  68.9   1.0   0.9  1.9  0.8  0.6  0.1  1.0          1.0  \n",
       "4  1.9  67.4   1.0   1.5  2.5  0.3  0.3  0.4  0.8          1.0  \n",
       "5  0.5  73.2   0.2   0.7  0.8  1.8  0.4  0.0  0.7          0.0  \n",
       "6  1.8  81.1   0.5   1.4  2.0  0.6  0.2  0.1  0.7          1.0  \n",
       "7  0.8  87.5   0.8   0.9  1.7  0.2  0.2  0.1  0.7          1.0  \n",
       "8  0.5  71.4   0.2   0.6  0.8  2.3  0.3  0.0  1.1          0.0  \n",
       "9  1.4  67.8   0.4   0.7  1.1  0.3  0.2  0.0  0.7          0.0  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"nba_logreg.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot to say about the preprocessing of this dataset, but we will remain soft:\n",
    "* There exist some *NA* in the column *3P%* because of a division by zero, but the correct value is 0\n",
    "* Some names are displayed multiple times, with exact same statistics but different labels. We decided to remove these observations\n",
    "* We create three new features based on known advanced statistics in the NBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"3P%\"] = df[\"3P%\"].fillna(0)\n",
    "unique_names = pd.DataFrame({\"Name\": df[\"Name\"].value_counts().loc[df[\"Name\"].value_counts() == 1, ].index})\n",
    "df = df.merge(unique_names, on=\"Name\", how=\"inner\")\n",
    "df[\"TSA\"] = df[\"FGA\"] + 0.44 * df[\"FTA\"]\n",
    "df[\"TS%\"] = df[\"PTS\"]/ (2 * df[\"TSA\"])\n",
    "df[\"3NG\"] = 1.94 * df[\"3P Made\"] - 1.06 * (df[\"3PA\"] - df[\"3P Made\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the use of the **Assistant** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from Jarvis import Assistant\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "metrics = [accuracy_score, precision_score, recall_score, f1_score]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=[\"Name\", \"TARGET_5Yrs\"], axis=1)\n",
    "y = df[\"TARGET_5Yrs\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, train_size=0.8)\n",
    "\n",
    "jarvis = Assistant(X_train, y_train, f1_score, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at the performance of a standard logistic regression, without tuning :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "accuracy: 0.71 (+/-0.04)\t precision: 0.75 (+/-0.03)\t recall: 0.81 (+/-0.06)\t f1: 0.78 (+/-0.04)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "jarvis.tryout(LogisticRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite good ! But at the moment, no model has yet been added to the assistant. We need to learn the fine-tuned model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "accuracy: 0.72 (+/-0.06)\t precision: 0.75 (+/-0.04)\t recall: 0.83 (+/-0.07)\t f1: 0.79 (+/-0.05)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "jarvis.learn([(LogisticRegression, {\"C\": [1e-6, 1e-5, 1e-4, 1e-3, 0.01, 0.1],\n",
    "                                    \"solver\": [\"lbfgs\"], \n",
    "                                    \"max_iter\": [1000]})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assistant stored the best model according to the cross-validated grid search we performed. Let's add other algorithm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "accuracy: 0.72 (+/-0.06)\t precision: 0.75 (+/-0.04)\t recall: 0.83 (+/-0.07)\t f1: 0.79 (+/-0.05)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "DecisionTreeClassifier\n",
      "accuracy: 0.66 (+/-0.06)\t precision: 0.71 (+/-0.04)\t recall: 0.77 (+/-0.08)\t f1: 0.73 (+/-0.05)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "RandomForestClassifier\n",
      "accuracy: 0.71 (+/-0.05)\t precision: 0.74 (+/-0.03)\t recall: 0.81 (+/-0.06)\t f1: 0.77 (+/-0.04)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "LGBMClassifier\n",
      "accuracy: 0.71 (+/-0.03)\t precision: 0.74 (+/-0.02)\t recall: 0.82 (+/-0.04)\t f1: 0.78 (+/-0.03)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "params_logit = {\"C\": [1e-6, 1e-5, 1e-4, 1e-3, 0.01, 0.1], \n",
    "                \"solver\": [\"lbfgs\"], \n",
    "                \"max_iter\": [5000]}\n",
    "\n",
    "params_tree = {\"max_depth\": [7, 8, 9, 10, 11, 12, 13, 14, 15], \n",
    "               \"min_samples_leaf\": [1, 2, 3, 4, 5]}\n",
    "\n",
    "params_forest = {\"n_estimators\": [10, 20, 30, 40, 50, 75, 100], \n",
    "                 \"max_depth\": [6, 8, 10, 12, 14]}\n",
    "\n",
    "params_LGBM = {\"n_estimators\": [10, 20, 30, 40, 50, 75, 100], \n",
    "               \"max_depth\": [6, 8, 10, 12, 14], \"lr\": [1e-3, 1e-2, 0.1, 0.5, 1, 1.5]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "models = [(LogisticRegression, params_logit), \n",
    "          (DecisionTreeClassifier, params_tree), \n",
    "          (RandomForestClassifier, params_forest),\n",
    "          (LGBMClassifier, params_LGBM)\n",
    "         ]\n",
    "\n",
    "\n",
    "jarvis.learn(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we tried first the best performing algorithm ! Let's have a recap of all the model stored already :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: LogisticRegression - LogisticRegression(C=0.1, max_iter=1000)\n",
      "accuracy: 0.72 (+/-0.06)\t precision: 0.75 (+/-0.04)\t recall: 0.83 (+/-0.07)\t f1: 0.79 (+/-0.05)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "id: LogisticRegression_1 - LogisticRegression(C=0.1, max_iter=5000)\n",
      "accuracy: 0.72 (+/-0.06)\t precision: 0.75 (+/-0.04)\t recall: 0.83 (+/-0.07)\t f1: 0.79 (+/-0.05)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "id: DecisionTreeClassifier - DecisionTreeClassifier(max_depth=7, min_samples_leaf=5)\n",
      "accuracy: 0.66 (+/-0.05)\t precision: 0.72 (+/-0.03)\t recall: 0.75 (+/-0.09)\t f1: 0.73 (+/-0.05)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "id: RandomForestClassifier - RandomForestClassifier(max_depth=6)\n",
      "accuracy: 0.70 (+/-0.06)\t precision: 0.74 (+/-0.03)\t recall: 0.80 (+/-0.06)\t f1: 0.77 (+/-0.04)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "id: LGBMClassifier - LGBMClassifier(lr=0.001, max_depth=8, n_estimators=20)\n",
      "accuracy: 0.71 (+/-0.03)\t precision: 0.74 (+/-0.02)\t recall: 0.82 (+/-0.04)\t f1: 0.78 (+/-0.03)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "jarvis.performance_recap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two logistic regression stored, with the exact same performance. Also, the performance for the Random Forest is not as good as the others, then we decide to delete both of these two algorithms from the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarvis.delete_model([\"LogisticRegression_1\", \"DecisionTreeClassifier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: LogisticRegression - LogisticRegression(C=0.1, max_iter=1000)\n",
      "accuracy: 0.72 (+/-0.06)\t precision: 0.75 (+/-0.04)\t recall: 0.83 (+/-0.07)\t f1: 0.79 (+/-0.05)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "id: RandomForestClassifier - RandomForestClassifier(max_depth=6)\n",
      "accuracy: 0.71 (+/-0.06)\t precision: 0.74 (+/-0.05)\t recall: 0.81 (+/-0.07)\t f1: 0.76 (+/-0.04)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "id: LGBMClassifier - LGBMClassifier(lr=0.001, max_depth=8, n_estimators=20)\n",
      "accuracy: 0.71 (+/-0.03)\t precision: 0.74 (+/-0.02)\t recall: 0.82 (+/-0.04)\t f1: 0.78 (+/-0.03)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "jarvis.performance_recap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does ! Now, we want to compute a voting classifier from these three algorithm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarvis.make_ensemble()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: LogisticRegression - LogisticRegression(C=0.1, max_iter=1000)\n",
      "accuracy: 0.72 (+/-0.06)\t precision: 0.75 (+/-0.04)\t recall: 0.83 (+/-0.07)\t f1: 0.79 (+/-0.05)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "id: RandomForestClassifier - RandomForestClassifier(max_depth=6)\n",
      "accuracy: 0.70 (+/-0.06)\t precision: 0.74 (+/-0.03)\t recall: 0.81 (+/-0.06)\t f1: 0.76 (+/-0.04)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "id: LGBMClassifier - LGBMClassifier(lr=0.001, max_depth=8, n_estimators=20)\n",
      "accuracy: 0.71 (+/-0.03)\t precision: 0.74 (+/-0.02)\t recall: 0.82 (+/-0.04)\t f1: 0.78 (+/-0.03)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "id: VotingClassifier - VotingClassifier(estimators=[('LogisticRegression',\n",
      "                              LogisticRegression(C=0.1, max_iter=1000)),\n",
      "                             ('RandomForestClassifier',\n",
      "                              RandomForestClassifier(max_depth=6)),\n",
      "                             ('LGBMClassifier',\n",
      "                              LGBMClassifier(lr=0.001, max_depth=8,\n",
      "                                             n_estimators=20))])\n",
      "accuracy: 0.71 (+/-0.04)\t precision: 0.75 (+/-0.02)\t recall: 0.81 (+/-0.04)\t f1: 0.78 (+/-0.03)\t \n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "jarvis.performance_recap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble model has a low variance, as expected from the theory, but high performance overall. We have now seen everything for learning and testing algorithms.\n",
    "We need to predict on an unseen test dataset. We can pick each algorith stored. Let's try :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "'Logit' is not in the model id list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/Projet Jarvis/Jarvis.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model_id, X_test, predict_proba)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-029cb461b09c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjarvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Projet Jarvis/Jarvis.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model_id, X_test, predict_proba)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnboundLocalError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"'%s' is not in the model id list\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: 'Logit' is not in the model id list"
     ]
    }
   ],
   "source": [
    "y_pred = jarvis.predict(\"Logit\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the error said, the identifier of the model is not correct. We add this error because it is way more understandable than the other error.\n",
    "This time with a valid id :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score : 0.7929\n"
     ]
    }
   ],
   "source": [
    "y_pred = jarvis.predict(\"VotingClassifier\", X_test)\n",
    "print(\"F1-score : %0.4f\" % f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is in the interval of performance predicted, perfect !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
